{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**NLP** **Techniques**"
      ],
      "metadata": {
        "id": "J0Ph8e2eOeF3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jv-PuJUeOTo_"
      },
      "outputs": [],
      "source": [
        "import nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Tokenization**"
      ],
      "metadata": {
        "id": "euV-cny4V9N8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')\n",
        "#required for task completed word_tokenize and sent_tokenize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2XvY4vWYO4K",
        "outputId": "4ee71078-f14f-4715-e15a-2eec8d73aa78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize, sent_tokenize"
      ],
      "metadata": {
        "id": "tnaKAsZ5V8fn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"hello openeyes software solution private limited\"\n",
        "#word tokenize\n",
        "print(word_tokenize(text))\n",
        "#word=word_tokenize(text)\n",
        "#print(\"words:\",word)\n",
        "\n",
        "#sent tokenize\n",
        "print(sent_tokenize(text))\n",
        "#sent=sent_tokenize(text)\n",
        "#print(\"sentences:\",sent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0nyT2XnWsF4",
        "outputId": "738a5168-b972-43b8-dd09-5f9fbd063854"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hello', 'openeyes', 'software', 'solution', 'private', 'limited']\n",
            "['hello openeyes software solution private limited']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Converting lower case **"
      ],
      "metadata": {
        "id": "UIXGF10iaNEa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"HELLO  ItiSha SAkhReliya\"\n",
        "lowercased_text = text.lower()\n",
        "print(lowercased_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-kWex_aaF-3",
        "outputId": "41a0ee12-f2e5-4634-a5d3-d35cde0a4798"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello  itisha sakhreliya\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stop Word Removal**"
      ],
      "metadata": {
        "id": "s5J6heS4as2G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0ssdEFOarof",
        "outputId": "5cba2d73-1be7-4ca4-bb52-88b0283f4af0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "filtered_sentence = ' '.join([word for word in sentence.split() if word.lower() not in stop_words])\n",
        "\n",
        "print(filtered_sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfqyLud6bVAi",
        "outputId": "87f8d922-77b7-4b41-b9ea-12455ec60e9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "quick brown fox jumps lazy dog.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2**.**stremming**"
      ],
      "metadata": {
        "id": "nK9RVRFzZM6s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "7O3uHGkmZgt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ps = PorterStemmer()\n",
        "words = [\"program\", \"programs\", \"programmer\", \"programming\", \"programmers\"]\n",
        "\n",
        "for w in words:\n",
        "\tprint(w, \" : \", ps.stem(w))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPtWzSOKUUC7",
        "outputId": "7a339ea7-4e90-4caa-ed37-e530291eb076"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "program  :  program\n",
            "programs  :  program\n",
            "programmer  :  programm\n",
            "programming  :  program\n",
            "programmers  :  programm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ps = PorterStemmer()\n",
        "\n",
        "sentence = \"Programmers program with programming languages\"\n",
        "words = word_tokenize(sentence)\n",
        "\n",
        "for w in words:\n",
        "\tprint(w, \" : \", ps.stem(w))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qq19dcljUWvN",
        "outputId": "835dd4d7-5bfc-490b-dbc4-e9938c5abbae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Programmers  :  programm\n",
            "program  :  program\n",
            "with  :  with\n",
            "programming  :  program\n",
            "languages  :  languag\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3**. **Lemmatization**"
      ],
      "metadata": {
        "id": "u52Xq9j0fylE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFvzwoDTgKkl",
        "outputId": "011b0717-18df-44da-ff92-473b527c279e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\"))\n",
        "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\"))\n",
        "print(\"better :\", lemmatizer.lemmatize(\"better\", pos=\"a\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8cMXnJifxhQ",
        "outputId": "eed09f30-f7e8-44f0-a273-0be73cb092b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rocks : rock\n",
            "corpora : corpus\n",
            "better : good\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. part of speech(pos)**"
      ],
      "metadata": {
        "id": "1Wh_Rnv-gfQL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RE2XPhIg3Gm",
        "outputId": "7dd02c77-d4bb-40e5-a020-8ec45806aebf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"hello openeyes software solution private limited\"\n",
        "#if already tokenize direct used\n",
        "print(\"pos_tags:\",pos_tag(word))\n",
        "text1=\"Preprocessing is crucial to clean and prepare the raw text data for analysis.\"\n",
        "#if not do first tokenize words\n",
        "tokens = word_tokenize(text1)\n",
        "print(\"pos_tags:\",pos_tag(tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLaMzoCBgU1f",
        "outputId": "617f51c2-8bb3-4a09-837d-8faa82b78870"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pos_tags: [('hello', 'JJ'), ('openeyes', 'NNS'), ('software', 'NN'), ('solution', 'NN'), ('private', 'JJ'), ('limited', 'VBD')]\n",
            "pos_tags: [('Preprocessing', 'VBG'), ('is', 'VBZ'), ('crucial', 'JJ'), ('to', 'TO'), ('clean', 'VB'), ('and', 'CC'), ('prepare', 'VB'), ('the', 'DT'), ('raw', 'JJ'), ('text', 'NN'), ('data', 'NNS'), ('for', 'IN'), ('analysis', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Named Entity Recognition (NER)**"
      ],
      "metadata": {
        "id": "uBKlDokliXpN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import ne_chunk\n",
        "nltk.download('maxent_ne_chunker_tab')\n",
        "nltk.download('words')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qTuXXnDkQC0",
        "outputId": "8450ae04-45fc-439b-c643-f0b2f060a955"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Barack Obama was born in Hawaii.\"\n",
        "tokens = word_tokenize(text)\n",
        "tagged = pos_tag(tokens)\n",
        "named_entities = ne_chunk(tagged)\n",
        "print(named_entities)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NWf1VVJ-gU2l",
        "outputId": "49bfca5a-6eb4-49ee-c920-c00344b6f0d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (PERSON Barack/NNP)\n",
            "  (PERSON Obama/NNP)\n",
            "  was/VBD\n",
            "  born/VBN\n",
            "  in/IN\n",
            "  (GPE Hawaii/NNP)\n",
            "  ./.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Removing Special Characters and Numbers**"
      ],
      "metadata": {
        "id": "hj1SjAmNm-s4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "text = \"I have 2 apples @! and i have 5 oranges ,can you given 1 apple and 2 oranges?/,,\"\n",
        "clean_text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "print(clean_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9igPbV8m_2r",
        "outputId": "4274cd2c-42f7-4d95-bc47-ed7247198228"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I have  apples  and i have  oranges can you given  apple and  oranges\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6. Vectorization (Converting Text to Numbers)**"
      ],
      "metadata": {
        "id": "OkgfT0G2k2u9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6.1 Bag of Words(BoW)**"
      ],
      "metadata": {
        "id": "AxV9dgG5pPx2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "j15-m70Pp7BI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# corpus is list of documents\n",
        "corpus = ['I love programming', 'Python is amazing', 'I love Python']\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "print(X.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyJaf02Bk182",
        "outputId": "00cbb785-2d30-45ae-8965-e2c29a95a4e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 1 1 0]\n",
            " [1 1 0 0 1]\n",
            " [0 0 1 0 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# corpus is list of documents\n",
        "corpus = ['Cats are cute', 'I love cats', 'Cats are playful']\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "print(X.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2cjdeqqGhTW",
        "outputId": "e06bf1b5-d04a-4a29-d118-56cf963a2314"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 1 1 0 0]\n",
            " [0 1 0 1 0]\n",
            " [1 1 0 0 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "6tyy_LMBrAyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " **6.2 Term Frequency-Inverse Document Frequency (TF-IDF)**"
      ],
      "metadata": {
        "id": "YF1ht6DDqIPk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = ['I love programming', 'Python is amazing', 'I love Python']\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "print(X.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGLvErquq3ER",
        "outputId": "a6a706cc-2a0f-4689-f505-f1918c3c2dbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.         0.60534851 0.79596054 0.        ]\n",
            " [0.62276601 0.62276601 0.         0.         0.4736296 ]\n",
            " [0.         0.         0.70710678 0.         0.70710678]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6.3 Continuous bag of words (CBOW)**"
      ],
      "metadata": {
        "id": "hU-06dHEr6d5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim\n",
        "from gensim.models import Word2Vec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6e-iOtCse8M",
        "outputId": "d9d6eafb-a652-46cb-e94f-27e0e2b5b017"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.5)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [['i', 'love', 'programming'], ['python', 'is', 'amazing']]\n",
        "model = Word2Vec(sentences, min_count=1)\n",
        "\n",
        "vector = model.wv['python']\n",
        "\n",
        "print(vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hyPI_nOq3Qy",
        "outputId": "159be089-4dde-4dbf-fd61-5832c9cf3800"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 9.4563962e-05  3.0773198e-03 -6.8126451e-03 -1.3754654e-03\n",
            "  7.6685809e-03  7.3464094e-03 -3.6732971e-03  2.6427018e-03\n",
            " -8.3171297e-03  6.2054861e-03 -4.6373224e-03 -3.1641065e-03\n",
            "  9.3113566e-03  8.7338570e-04  7.4907029e-03 -6.0740625e-03\n",
            "  5.1605068e-03  9.9228229e-03 -8.4573915e-03 -5.1356913e-03\n",
            " -7.0648370e-03 -4.8626517e-03 -3.7785638e-03 -8.5361991e-03\n",
            "  7.9556061e-03 -4.8439382e-03  8.4236134e-03  5.2625705e-03\n",
            " -6.5500261e-03  3.9578713e-03  5.4701497e-03 -7.4265362e-03\n",
            " -7.4057197e-03 -2.4752307e-03 -8.6257253e-03 -1.5815723e-03\n",
            " -4.0343284e-04  3.2996845e-03  1.4418805e-03 -8.8142155e-04\n",
            " -5.5940580e-03  1.7303658e-03 -8.9737179e-04  6.7936908e-03\n",
            "  3.9735902e-03  4.5294715e-03  1.4343059e-03 -2.6998555e-03\n",
            " -4.3668128e-03 -1.0320747e-03  1.4370275e-03 -2.6460087e-03\n",
            " -7.0737829e-03 -7.8053069e-03 -9.1217868e-03 -5.9351693e-03\n",
            " -1.8474245e-03 -4.3238713e-03 -6.4606704e-03 -3.7173224e-03\n",
            "  4.2891586e-03 -3.7390434e-03  8.3781751e-03  1.5339935e-03\n",
            " -7.2423196e-03  9.4337985e-03  7.6312125e-03  5.4932819e-03\n",
            " -6.8488456e-03  5.8226790e-03  4.0090932e-03  5.1853694e-03\n",
            "  4.2559016e-03  1.9397545e-03 -3.1701624e-03  8.3538452e-03\n",
            "  9.6121803e-03  3.7926030e-03 -2.8369951e-03  7.1275235e-06\n",
            "  1.2188185e-03 -8.4583247e-03 -8.2239453e-03 -2.3101569e-04\n",
            "  1.2372875e-03 -5.7433806e-03 -4.7252737e-03 -7.3460746e-03\n",
            "  8.3286157e-03  1.2129784e-04 -4.5093987e-03  5.7017053e-03\n",
            "  9.1800150e-03 -4.0998720e-03  7.9646818e-03  5.3754342e-03\n",
            "  5.8791232e-03  5.1259040e-04  8.2130842e-03 -7.0190406e-03]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **unigram,bigram,trigram,Ngram**"
      ],
      "metadata": {
        "id": "Ttqms-c1tKgM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import ngrams\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Tokenize the text into words\n",
        "tokens = word_tokenize(text1)\n",
        "\n",
        "#  Unigrams (1-grams)\n",
        "unigrams = list(ngrams(tokens, 1))\n",
        "print(\"Unigrams:\", unigrams)\n",
        "\n",
        "#  Bigrams (2-grams)\n",
        "bigrams = list(ngrams(tokens, 2))\n",
        "print(\"Bigrams:\", bigrams)\n",
        "\n",
        "#  Trigrams (3-grams)\n",
        "trigrams = list(ngrams(tokens, 3))\n",
        "print(\"Trigrams:\", trigrams)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zhb59rQqtIQc",
        "outputId": "4d8f39e2-04f3-466d-ab89-c1a21029f136"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unigrams: [('Preprocessing',), ('is',), ('crucial',), ('to',), ('clean',), ('and',), ('prepare',), ('the',), ('raw',), ('text',), ('data',), ('for',), ('analysis',), ('.',)]\n",
            "Bigrams: [('Preprocessing', 'is'), ('is', 'crucial'), ('crucial', 'to'), ('to', 'clean'), ('clean', 'and'), ('and', 'prepare'), ('prepare', 'the'), ('the', 'raw'), ('raw', 'text'), ('text', 'data'), ('data', 'for'), ('for', 'analysis'), ('analysis', '.')]\n",
            "Trigrams: [('Preprocessing', 'is', 'crucial'), ('is', 'crucial', 'to'), ('crucial', 'to', 'clean'), ('to', 'clean', 'and'), ('clean', 'and', 'prepare'), ('and', 'prepare', 'the'), ('prepare', 'the', 'raw'), ('the', 'raw', 'text'), ('raw', 'text', 'data'), ('text', 'data', 'for'), ('data', 'for', 'analysis'), ('for', 'analysis', '.')]\n"
          ]
        }
      ]
    }
  ]
}